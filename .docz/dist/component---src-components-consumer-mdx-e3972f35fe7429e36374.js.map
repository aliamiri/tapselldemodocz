{"version":3,"sources":["webpack:///../src/components/Consumer.mdx"],"names":["_frontmatter","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","isMDXComponent"],"mappings":"yPAQaA,G,UAAe,S,6MAC5B,IAAMC,EAAc,CAClBD,gBAEIE,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,EACF,8BACD,OAAO,YAACJ,EAAD,eAAeD,EAAiBK,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAG5E,kBACE,GAAM,WADR,WAGA,4aACA,sBACE,kBAAIC,WAAW,MACb,iBAAGA,WAAW,MAAK,sBAAQA,WAAW,KAAnB,gBAAnB,2KAA+O,kBAAIA,WAAW,MAA9P,kfAGF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,sBAAQA,WAAW,KAAnB,QAAnB,0N,2MAORJ,EAAWK,gBAAiB","file":"component---src-components-consumer-mdx-e3972f35fe7429e36374.js","sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/iraj/Projects/Tapsell/docz/my-docz-app/node_modules/gatsby-theme-docz/src/base/Layout.js\";\nexport const _frontmatter = {};\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h1 {...{\n      \"id\": \"cosumer\"\n    }}>{`Cosumer`}</h1>\n    <p>{`Kafka Consumer or big data handler is where Kafka's data consumption and Cassandra's data manipulation occurs. Some simple data processing takes place in this part of the project, and I've tried to explain them with inline comments on code. Besides these data processing, this module uses Cassandra for data storage and will make changes to those data if necessary. Two more things are worth mentioning:`}</p>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><strong parentName=\"p\">{`Kafka stream`}</strong>{` One of this project's main requirements was to consume Kafka messages regularly, and I think Kafka stream is an excellent candidate to handle this part of the project.`}<br parentName=\"p\"></br>{`\n`}{`Kafka stream commits the last read index regularly, and it may cause some problems when for example, the program suddenly crashes. We have processed some messages though we didn't have the time to commit, so when the program boots up again, Kafka thinks that our client didn't receive those messages, and we will consume those messages for the second time.  I'll double-check the message with the database in this project, but I don't think this approach is applicable for every scenario.`}</p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><strong parentName=\"p\">{`Test`}</strong>{` To write the test, I've used EmbeddedKafka three simple scenarios, Creation of AdEvent, Receiving an acceptable ClickEvent, and receiving an unacceptable Click event which passed all of them successfully. `}</p>\n      </li>\n    </ul>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}